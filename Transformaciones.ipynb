{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b9/7yv8wf7j1xd39p1ghzll0fqh0000gn/T/ipykernel_78260/3571106454.py:2: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_csv_files_in_directory(directorio,outputs):\n",
    "    for directorio, archivo_salida in zip(directorio, outputs):\n",
    "        dfs = []\n",
    "        for filename in os.listdir(directorio):\n",
    "            # Verifica que sean archivos CSV\n",
    "            if filename.endswith('.csv'):\n",
    "                filepath = os.path.join(directorio, filename)\n",
    "                df = pd.read_csv(filepath)\n",
    "                dfs.append(df)\n",
    "        merged_df = dfs[0]\n",
    "        \n",
    "        for df in dfs[1:]:\n",
    "            merged_df = pd.merge(merged_df, df, on=['ID_RIO', 'coord_TRAMO'], how='inner')\n",
    "        \n",
    "        print(merged_df)\n",
    "        print(\"Ahora guardamos en local\")\n",
    "        merged_df.to_csv(archivo_salida, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verificar_permisos_directorio(directory):\n",
    "    # Verificar si el directorio existe\n",
    "    if not os.path.exists(directory):\n",
    "        print(f\"El directorio {directory} no existe.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar permisos de lectura\n",
    "    if not os.access(directory, os.R_OK):\n",
    "        print(f\"No tienes permisos de lectura para el directorio {directory}.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar permisos de escritura\n",
    "    if not os.access(directory, os.W_OK):\n",
    "        print(f\"No tienes permisos de escritura para el directorio {directory}.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar permisos de ejecución\n",
    "    if not os.access(directory, os.X_OK):\n",
    "        print(f\"No tienes permisos de ejecución para el directorio {directory}.\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"Tienes permisos completos para acceder al directorio {directory}.\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from shapely.wkt import loads\n",
    "from shapely.geometry import LineString\n",
    "import geopandas as gpd\n",
    "import time\n",
    "\n",
    "def calcular_distancias(fich_rios, contaminante_sf, fich_output, contaminante_col):\n",
    "    for idx, rio in fich_rios.iterrows():\n",
    "        linestring = rio['WKT']\n",
    "        geometry = loads(linestring)\n",
    "        \n",
    "        for part in geometry.geoms:\n",
    "            #Se reducen el número de tramos a 3\n",
    "            coords = list(part.coords)\n",
    "            coordinates_to_check = [coords[0], coords[len(coords)//2] if len(coords) % 2 != 0 else coords[len(coords)//2 - 1], coords[-1]]\n",
    "            #print(\"Coordinates to check:\",coordinates_to_check)\n",
    "            #print(f\"Número de tramos del rio simplificados: {len(coordinates_to_check)}\")\n",
    "            for coordinate in coordinates_to_check:\n",
    "                #print(\"Calculando el sensor más cercano al punto: \",coordinate)\n",
    "                distances = []\n",
    "                for idx, row in contaminante_sf.iterrows():\n",
    "                    sensor = row.geometry.coords[0]  # Obtenemos las coordenadas del sensor\n",
    "                    distance = np.sqrt((coordinate[0] - sensor[0]) ** 2 + (coordinate[1] - sensor[1]) ** 2)\n",
    "                    distances.append(distance)\n",
    "\n",
    "                min_distance = min(distances)\n",
    "                min_distance_index = distances.index(min_distance)\n",
    "                closest_sensor = contaminante_sf.iloc[min_distance_index]\n",
    "\n",
    "                # Crear un DataFrame con los datos\n",
    "                data = {\n",
    "                    'ID_RIO': [rio['PFAFRIO']],\n",
    "                    'coord_TRAMO': [coordinate],\n",
    "                    f'valor_{contaminante_col}': [closest_sensor['PromedioDe']]\n",
    "                }\n",
    "                info_tramo = pd.DataFrame(data)\n",
    "\n",
    "                # Agregar datos al archivo CSV y Verificar si el archivo existe\n",
    "                if os.path.exists(fich_output):\n",
    "                    header = False\n",
    "                else:\n",
    "                    header = True\n",
    "\n",
    "                # Escribir datos en el archivo CSV\n",
    "                info_tramo.to_csv(fich_output, mode='a', header=header, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rios_A = pd.read_csv(\"/Users/silviadelatorre/Desktop/TFG/FICHEROS INPUT/RiosAtlantico.csv\")\n",
    "print(f'Numero de rios Atlantico: {df_rios_A.shape[0]}')\n",
    "df_rios_M = pd.read_csv(\"/Users/silviadelatorre/Desktop/TFG/FICHEROS INPUT/RiosMediterraneo.csv\")\n",
    "print(f'Numero de rios Mediterraneo: {df_rios_M.shape[0]}')\n",
    "df_rios_A_gdf = gpd.GeoDataFrame(df_rios_A, geometry=gpd.GeoSeries.from_wkt(df_rios_A['WKT']))\n",
    "df_fitobentos = pd.read_csv(\"/Users/silviadelatorre/Desktop/TFG/AGENTES CONTAMINANTES/Fitobentos.csv\")\n",
    "fitobentos_gdf = gpd.GeoDataFrame(df_fitobentos, geometry=gpd.GeoSeries.from_wkt(df_fitobentos['WKT']))\n",
    "output_folder_a = \"/Users/silviadelatorre/Desktop/TFG/PFG/Results/VALORES CONTAMINACION/ATLANTICO\"\n",
    "output_path = os.path.join(output_folder_a, \"Fitobentos.csv\")\n",
    "calcular_distancias(df_rios_A_gdf, fitobentos_gdf,output_path,\"Fitobentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNIR TABLAS EN UNA SOLA\n",
    "# Directorios de vertientes\n",
    "vertientes_directorios = ['/Users/silviadelatorre/Desktop/TFG/PFG/Results/VALORES CONTAMINACION/ATLANTICO', '/Users/silviadelatorre/Desktop/TFG/PFG/Results/VALORES CONTAMINACION/MEDITERRANEO']\n",
    "output_files = [\"/Users/silviadelatorre/Desktop/TFG/PFG/Results/VALORES CONTAMINACION/ATLANTICO/ContaminacionAtlantico.csv\",\"/Users/silviadelatorre/Desktop/TFG/PFG/Results/VALORES CONTAMINACION/MEDITERRANEO/ContaminacionMediterraneo.csv\"]\n",
    "\n",
    "merged_df = merge_csv_files_in_directory(vertientes_directorios,output_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONCATENAR FICHEROS DE LAS VERTIENTS\n",
    "def merge_agents(directory, agent_name):\n",
    "    dfs = []\n",
    "    for vertiente in ['ATLANTICO', 'MEDITERRANEO']:\n",
    "        vertiente_path = os.path.join(directory, vertiente)\n",
    "        agent_path = os.path.join(vertiente_path, f'{agent_name}.csv')\n",
    "        if os.path.exists(agent_path):\n",
    "            df = pd.read_csv(agent_path)\n",
    "            print(f'Numero de filas {vertiente} en {agent_name} son {df.shape[0]}')\n",
    "            dfs.append(df)\n",
    "    merged_df = pd.concat(dfs)\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio base donde se encuentran las carpetas ATLANTICO y MEDITERRANEO\n",
    "base_directory = \"/Users/silviadelatorre/Desktop/TFG/PFG/Results/VALORES CONTAMINACION\"\n",
    "\n",
    "# Lista de nombres de agentes contaminantes\n",
    "agentes_contaminantes = ['Nitrato', 'Amonio', 'Fosforo', 'Fosfato','Fitobentos','Grado Trofico']\n",
    "\n",
    "# # Iterar sobre cada agente contaminante\n",
    "# for agente in agentes_contaminantes:\n",
    "#     # Combinar los archivos correspondientes a cada agente contaminante\n",
    "#     merged_df = merge_agents(base_directory, agente)\n",
    "#     # Guardar el DataFrame combinado en un nuevo archivo CSV\n",
    "#     output_path = os.path.join(base_directory, f'{agente}_merged.csv')\n",
    "#     merged_df.to_csv(output_path, index=False)\n",
    "#     print(f'Numero de filas en {agente} son {merged_df.shape[0]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
